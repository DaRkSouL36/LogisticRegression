{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be678f67",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ LOGISTIC REGRESSION WITH REGULARIZATION\n",
    "\n",
    "## WHAT IS REGULARIZATION?\n",
    "- IN REAL-WORLD MACHINE LEARNING, MODELS DO NOT FAIL BECAUSE THEY ARE TOO SIMPLE, THEY FAIL BECAUSE THEY **LEARN TOO MUCH**.\n",
    "- THIS PHENOMENON IS CALLED **OVERFITTING**.\n",
    "- REGULARIZATION IS A TECHNIQUE USED TO PREVENT **OVERFITTING** BY PENALIZING LARGE WEIGHTS ($\\theta$).\n",
    "- IT ADDS A `\"COMPLEXITY TERM\"` TO THE COST FUNCTION, FORCING THE MODEL TO KEEP PARAMETERS SMALL AND SIMPLE.\n",
    "\n",
    "> OVERFITTING OCCURS WHEN A MODEL:\n",
    "- MEMORIZES TRAINING DATA\n",
    "- LEARNS NOISE INSTEAD OF PATTERNS\n",
    "- PERFORMS WELL ON TRAIN DATA BUT FAILS ON UNSEEN DATA\n",
    "\n",
    "> REGULARIZATION IS A **CONTROL MECHANISM** THAT:\n",
    "- PENALIZES LARGE WEIGHTS\n",
    "- LIMITS MODEL COMPLEXITY\n",
    "- IMPROVES GENERALIZATION\n",
    "- INTRODUCES BIAS TO REDUCE VARIANCE\n",
    "\n",
    "LOGISTIC REGRESSION LEARNS PARAMETERS BY MINIMIZING A COST FUNCTION.\n",
    "\n",
    "WITHOUT REGULARIZATION:\n",
    "\n",
    "## REGULARIZATION IN LOGISTIC REGRESSION\n",
    "\n",
    "COST FUNCTION WITHOUT REGULARIZATION \n",
    "\n",
    "    J(Î¸) = -(1/m) Î£ [yÂ·log(hÎ¸(x)) + (1 âˆ’ y)Â·log(1 âˆ’ hÎ¸(x))]\n",
    "\n",
    "### THE THREE TYPES WE WILL IMPLEMENT:\n",
    "\n",
    "> **1. L2 REGULARIZATION (RIDGE)**\n",
    "> * **PENALTY:** SQUARED MAGNITUDE OF WEIGHTS ($\\theta_j^2$).\n",
    "> * **EFFECT:** SHRINKS ALL COEFFICIENTS TOWARDS ZERO EVENLY. GOOD FOR COLLINEARITY. SMOOTH DECISION BOUNDARY\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "> **2. L1 REGULARIZATION (LASSO)**\n",
    "> * **PENALTY:** ABSOLUTE VALUE OF WEIGHTS ($|\\theta_j|$).\n",
    "> * **EFFECT:** CAN SHRINK COEFFICIENTS TO EXACTLY ZERO. PERFORMS **FEATURE SELECTION**. CREATES SPARSE MODELS.\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "> **3. ELASTIC NET**\n",
    "> * **PENALTY:** A COMBINATION OF BOTH L1 AND L2.\n",
    "> * **EFFECT:** BALANCES SPARSITY (L1) AND STABILITY (L2).\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + r \\cdot (\\text{L1}) + (1-r) \\cdot (\\text{L2})$$\n",
    "\n",
    "---\n",
    "`NOTE`: WE DO NOT REGULARIZE THE BIAS TERM ($\\theta_0$). REGULARIZATION IS APPLIED **ONLY TO WEIGHTS.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
