{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be678f67",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ LOGISTIC REGRESSION WITH REGULARIZATION\n",
    "\n",
    "## WHAT IS REGULARIZATION?\n",
    "- IN REAL-WORLD MACHINE LEARNING, MODELS DO NOT FAIL BECAUSE THEY ARE TOO SIMPLE, THEY FAIL BECAUSE THEY **LEARN TOO MUCH**.\n",
    "- THIS PHENOMENON IS CALLED **OVERFITTING**.\n",
    "- REGULARIZATION IS A TECHNIQUE USED TO PREVENT **OVERFITTING** BY PENALIZING LARGE WEIGHTS ($\\theta$).\n",
    "- IT ADDS A `\"COMPLEXITY TERM\"` TO THE COST FUNCTION, FORCING THE MODEL TO KEEP PARAMETERS SMALL AND SIMPLE.\n",
    "\n",
    "> OVERFITTING OCCURS WHEN A MODEL:\n",
    "- MEMORIZES TRAINING DATA\n",
    "- LEARNS NOISE INSTEAD OF PATTERNS\n",
    "- PERFORMS WELL ON TRAIN DATA BUT FAILS ON UNSEEN DATA\n",
    "\n",
    "> REGULARIZATION IS A **CONTROL MECHANISM** THAT:\n",
    "- PENALIZES LARGE WEIGHTS\n",
    "- LIMITS MODEL COMPLEXITY\n",
    "- IMPROVES GENERALIZATION\n",
    "- INTRODUCES BIAS TO REDUCE VARIANCE\n",
    "\n",
    "LOGISTIC REGRESSION LEARNS PARAMETERS BY MINIMIZING A COST FUNCTION.\n",
    "\n",
    "WITHOUT REGULARIZATION:\n",
    "\n",
    "## REGULARIZATION IN LOGISTIC REGRESSION\n",
    "\n",
    "COST FUNCTION WITHOUT REGULARIZATION \n",
    "\n",
    "    J(Î¸) = -(1/m) Î£ [yÂ·log(hÎ¸(x)) + (1 âˆ’ y)Â·log(1 âˆ’ hÎ¸(x))]\n",
    "\n",
    "### THE THREE TYPES WE WILL IMPLEMENT:\n",
    "\n",
    "> **1. L2 REGULARIZATION (RIDGE)**\n",
    "> * **PENALTY:** SQUARED MAGNITUDE OF WEIGHTS ($\\theta_j^2$).\n",
    "> * **EFFECT:** SHRINKS ALL COEFFICIENTS TOWARDS ZERO EVENLY. GOOD FOR COLLINEARITY. SMOOTH DECISION BOUNDARY\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "> **2. L1 REGULARIZATION (LASSO)**\n",
    "> * **PENALTY:** ABSOLUTE VALUE OF WEIGHTS ($|\\theta_j|$).\n",
    "> * **EFFECT:** CAN SHRINK COEFFICIENTS TO EXACTLY ZERO. PERFORMS **FEATURE SELECTION**. CREATES SPARSE MODELS.\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "> **3. ELASTIC NET**\n",
    "> * **PENALTY:** A COMBINATION OF BOTH L1 AND L2.\n",
    "> * **EFFECT:** BALANCES SPARSITY (L1) AND STABILITY (L2).\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + r \\cdot (\\text{L1}) + (1-r) \\cdot (\\text{L2})$$\n",
    "\n",
    "---\n",
    "`NOTE`: WE DO NOT REGULARIZE THE BIAS TERM ($\\theta_0$). REGULARIZATION IS APPLIED **ONLY TO WEIGHTS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a937f9",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES\n",
    "WE WILL USE ONLY NUMPY, PANDAS, AND MATPLOTLIB/SEABORN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d976a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT NUMERICAL COMPUTATION LIBRARY\n",
    "import numpy as np\n",
    "\n",
    "# IMPORT DATA HANDLING LIBRARY\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT VISUALIZATION LIBRARIES\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cfb83",
   "metadata": {},
   "source": [
    "## VISUALIZATION STYLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b66ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP THE VISUALIZATION STYLE GLOBALLY TO MATCH YOUR REQUIRED FORMAT\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8db33",
   "metadata": {},
   "source": [
    "## LOADING DATA\n",
    "READING THE DATA FROM CSV FILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3becd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# IF HEADERS EXIST, REMOVE 'header=None'.\n",
    "\n",
    "# LOAD X DATA FROM CSV FILE\n",
    "X_raw = pd.read_csv('DATA/logisticX.csv', header=None).values\n",
    "\n",
    "# LOAD Y DATA FROM CSV FILE\n",
    "y_raw = pd.read_csv('DATA/logisticY.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645438fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SHAPE --> (100, 2)\n",
      "Y SHAPE --> (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# RESHAPING Y TO ENSURE IT IS A COLUMN VECTOR (M, 1)\n",
    "y_data = y_raw.reshape(-1, 1)\n",
    "\n",
    "# CHECKING SHAPE\n",
    "print(f\"X SHAPE --> {X_raw.shape}\")\n",
    "print(f\"Y SHAPE --> {y_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b01cb",
   "metadata": {},
   "source": [
    "## PREPROCESSING DATA\n",
    "WE WILL NORMALIZE THE DATA AND ADD AN INTERCEPT TERM (BIAS) TO X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecf9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NORMALIZATION (MIN-MAX SCALING)\n",
    "# THIS IS CRUCIAL FOR GRADIENT DESCENT CONVERGENCE\n",
    "# MATH: X_norm = (X - min) / (max - min)\n",
    "\n",
    "X_min = np.min(X_raw, axis = 0)\n",
    "X_max = np.max(X_raw, axis = 0)\n",
    "X_norm = (X_raw - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6034bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED X SHAPE: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# ADDING INTERCEPT TERM (COLUMN OF ONES) TO X\n",
    "# MATH: X = [1, x1, x2, ...]\n",
    "\n",
    "m = len(y_data)\n",
    "X_data = np.hstack((np.ones((m, 1)), X_norm))\n",
    "\n",
    "print(f\"PROCESSED X SHAPE: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe46a3a",
   "metadata": {},
   "source": [
    "## REGULARIZED LOGISTIC REGRESSION CLASS\n",
    "THIS CLASS HANDLES L1, L2, AND ELASTIC NET.\n",
    "\n",
    "`KEY UPDATES FROM PREVIOUS CODE`\n",
    "- **COST FUNCTION:** ADDED PENALTY TERMS.\n",
    "- **GRADIENT:** ADDED DERIVATIVE OF PENALTIES.\n",
    "    * DERIVATIVE OF $\\theta^2$ IS $2\\theta$.\n",
    "    * DERIVATIVE OF $|\\theta|$ IS $\\text{sgn}(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.5, iterations = 1000, penalty = 'None', lambda_ = 0.1, l1_ratio = 0.5):\n",
    "        self.lr = learning_rate\n",
    "        self.iter = iterations\n",
    "        self.penalty = penalty        # 'none', 'l1', 'l2', 'elasticnet'\n",
    "        self.lambda_ = lambda_        # REGULARIZATION STRENGTH\n",
    "        self.l1_ratio = l1_ratio      # FOR ElasticNet (0=L2, 1=L1)\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        epsilon = 1e-15\n",
    "        \n",
    "        # BASIC LOG LOSS\n",
    "        cost = -(1 / m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "        \n",
    "        # REGULARIZATION TERM (EXCLUDING BIAS theta[0])\n",
    "        # WE SLICE theta[1:] BECAUSE WE DON'T PENALIZE BIAS\n",
    "        \n",
    "        theta_noBias = self.theta[1:]\n",
    "        penalty_term = 0\n",
    "        \n",
    "        if self.penalty == 'L2':\n",
    "            penalty_term = (self.lambda_ / (2 * m)) * np.sum(np.square(theta_noBias))\n",
    "        \n",
    "        elif self.penalty == 'L1':\n",
    "            penalty_term = (self.lambda_ / m) * np.sum(np.abs(theta_noBias))\n",
    "       \n",
    "        elif self.penalty == 'ElasticNet':\n",
    "            L1 = np.sum(np.abs(theta_noBias))\n",
    "            L2 = np.sum(np.square(theta_noBias))\n",
    "            \n",
    "            # SKLEARN STYLE MIX: r * L1 + (1-r) * L2 (SCALED BY LAMBDA)\n",
    "            penalty_term = (self.lambda_ / m) * (self.l1_ratio * L1 + (1 - self.l1_ratio) *  0.5 * L2)\n",
    "            \n",
    "        return cost + penalty_term\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.theta = np.zeros((X.shape[1], 1))    \n",
    "        self.cost_history = []\n",
    "        m = len(y)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            #  HYPOTHESIS\n",
    "            h = self.sigmoid(np.dot(X, self.theta))\n",
    "            \n",
    "            # BASIC GRADIENT (BASE_GRAD)\n",
    "            # dJ/dTheta = (1/m) * X.T * (h - y)\n",
    "            gradient = (1 / m) * np.dot((X.T), (h - y))\n",
    "            \n",
    "            # ADDING REGULARIZATION TO GRADIENT\n",
    "            # WE CREATE A COPY OF THETA AND SET BIAS TO 0 SO IT DOESN'T GET PENALIZED\n",
    "            theta_reg = self.theta.copy()\n",
    "            theta_reg[0] = 0\n",
    "            \n",
    "            if self.penalty == 'L2':\n",
    "                # DERIVATIVE OF (lambda/2m * sum(theta^2)) -> (lambda/m) * theta\n",
    "                gradient += (self.lambda_ / m) * theta_reg\n",
    "            \n",
    "            elif self.penalty == 'L1':\n",
    "                # DERIVATIVE OF (lambda/m * sum|theta|) -> (lambda/m) * sign(theta)\n",
    "                gradient += (self.lambda_ / m) * np.sign(theta_reg)\n",
    "            \n",
    "            elif self.penalty == 'ElasticNet':\n",
    "                L1 = np.sign(theta_reg)\n",
    "                L2 = theta_reg\n",
    "                gradient += (self.lambda_ / m) * (self.l1_ratio * L1 + (1 - self.l1_ratio) * L2)\n",
    "                \n",
    "            # UPDATE\n",
    "            self.theta -= (self.lr * gradient)\n",
    "            self.cost_history.append(self.cost_function(X, y))\n",
    "            \n",
    "            # if i % 100 == 0:\n",
    "            #     print(f\"ITERATION {i} | COST: {self.cost_function(X, y):.6f}\") \n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19468360",
   "metadata": {},
   "source": [
    "# ðŸ§® WHY THE 0.5 MULTIPLIER IN \n",
    "\n",
    "> `penalty_term = (self.lambda_ / m) * (self.l1_ratio * L1 + (1 - self.l1_ratio) *  0.5 * L2)`?\n",
    "\n",
    "### 1. THE \"GRADIENT CANCELLATION\" TRICK\n",
    "THE PRIMARY REASON IS **CALCULUS CONVENIENCE**.\n",
    "\n",
    "WHEN WE OPTIMIZE OUR MODEL, WE NEED TO FIND THE **DERIVATIVE** (GRADIENT) OF THE COST FUNCTION WITH RESPECT TO THE WEIGHTS ($\\theta$).\n",
    "\n",
    "> **WITHOUT THE 0.5:**\n",
    "> IF THE L2 TERM IS SIMPLY $\\theta^2$:\n",
    "> $$\\frac{d}{d\\theta}(\\theta^2) = 2\\theta$$\n",
    "> THIS INTRODUCES AN EXTRA FACTOR OF **2** INTO OUR GRADIENT UPDATES, WHICH WE WOULD HAVE TO CARRY AROUND.\n",
    "\n",
    "> **WITH THE 0.5:**\n",
    "> IF WE DEFINE THE L2 TERM AS $\\frac{1}{2}\\theta^2$:\n",
    "> $$\\frac{d}{d\\theta}\\left(\\frac{1}{2}\\theta^2\\right) = \\frac{1}{2} \\cdot 2\\theta = \\theta$$\n",
    "> THE **2** AND THE **1/2** CANCEL EACH OTHER OUT PERFECTLY! \n",
    "\n",
    "THIS LEAVES US WITH A CLEANER GRADIENT: JUST THE WEIGHTS THEMSELVES ($\\theta$).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. CONSISTENCY WITH SCIKIT-LEARN\n",
    "SINCE WE AIM TO COMPARE OUR RESULTS WITH SCIKIT-LEARN, WE MUST MATCH THEIR MATHEMATICAL DEFINITION.\n",
    "\n",
    "SCIKIT-LEARN DEFINES THE ELASTIC NET COST FUNCTION AS:\n",
    "\n",
    "$$J(\\theta) = \\text{LogLoss} + \\alpha \\cdot \\rho \\cdot ||\\theta||_1 + \\frac{\\alpha \\cdot (1-\\rho)}{2} \\cdot ||\\theta||_2^2$$\n",
    "\n",
    "* $\\alpha$ (Alpha) corresponds to `lambda_`\n",
    "* $\\rho$ (Rho) corresponds to `l1_ratio`\n",
    "* NOTICE THE **DIVISION BY 2** IN THE L2 TERM.\n",
    "\n",
    "### 3. VERIFYING THE CODE MATCH\n",
    "LOOK AT HOW THIS AFFECTS OUR **GRADIENT CALCULATION** IN THE `train` METHOD:\n",
    "\n",
    "**IN COST FUNCTION:**\n",
    "```python\n",
    "# WE ADD THE 0.5 HERE\n",
    "penalty_term = ... + (1 - self.l1_ratio) * 0.5 * l2_term\n",
    "```\n",
    "\n",
    "### IN GRADIENT CALCULATION:\n",
    "```python\n",
    "# WE DO NOT MULTIPLY BY 2 HERE. WE JUST USE theta_reg DIRECTLY.\n",
    "# BECAUSE DERIVATIVE OF 0.5*theta^2 IS JUST theta.\n",
    "L2 = theta_reg\n",
    "gradient += ... + (1 - self.l1_ratio) * L2\n",
    "```\n",
    "\n",
    "> IF WE HAD REMOVED THE 0.5 FROM THE COST FUNCTION, WE WOULD HAVE NEEDED TO WRITE `2 * L2` IN THE GRADIENT CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe6483",
   "metadata": {},
   "source": [
    "## COMPARING MODELS: L1 VS L2 VS NONE\n",
    "WE WILL TRAIN THREE MODELS WITH THE SAME LEARNING RATE BUT DIFFERENT REGULARIZATION TECHNIQUES.\n",
    "\n",
    "WE USE A HIGH `LAMBDA` (REGULARIZATION STRENGTH) TO MAKE THE EFFECTS VISIBLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dab3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COEFFICIENT COMPARISON ---\n",
      "REGULARIZATION       THETA_0 (BIAS)  THETA_1         THETA_2        \n",
      "------------------------------------------------------------\n",
      "NONE                 0.3804          8.2186          -7.9268        \n",
      "L2                   0.2268          1.8947          -2.1675        \n",
      "L1                   0.4268          3.9379          -4.3647        \n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "LR = 0.5\n",
    "ITER = 1000\n",
    "# STRONG REGULARIZATION TO SHOW EFFECT\n",
    "LAMBDA_VAL = 2.0\n",
    "\n",
    "# NO REGULARIZATION\n",
    "model_None = RegularizedLogisticRegression(LR, ITER, penalty = 'None')\n",
    "model_None.train(X_data, y_data)\n",
    "\n",
    "# L2 (RIDGE)\n",
    "model_L2 = RegularizedLogisticRegression(LR, ITER, penalty='L2', lambda_=LAMBDA_VAL)\n",
    "model_L2.train(X_data, y_data)\n",
    "\n",
    "# L1 (LASSO)\n",
    "model_L1 = RegularizedLogisticRegression(LR, ITER, penalty='L1', lambda_=LAMBDA_VAL)\n",
    "model_L1.train(X_data, y_data)\n",
    "\n",
    "# PRINTING COEFFICIENTS TO SEE SHRINKAGE\n",
    "print(\"--- COEFFICIENT COMPARISON ---\")\n",
    "print(f\"{'REGULARIZATION':<20} {'THETA_0 (BIAS)':<15} {'THETA_1':<15} {'THETA_2':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'NONE':<20} {model_None.theta[0,0]:<15.4f} {model_None.theta[1,0]:<15.4f} {model_None.theta[2,0]:<15.4f}\")\n",
    "print(f\"{'L2':<20} {model_L2.theta[0,0]:<15.4f} {model_L2.theta[1,0]:<15.4f} {model_L2.theta[2,0]:<15.4f}\")\n",
    "print(f\"{'L1':<20} {model_L1.theta[0,0]:<15.4f} {model_L1.theta[1,0]:<15.4f} {model_L1.theta[2,0]:<15.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
