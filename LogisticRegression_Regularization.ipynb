{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be678f67",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ LOGISTIC REGRESSION WITH REGULARIZATION\n",
    "\n",
    "## WHAT IS REGULARIZATION?\n",
    "- IN REAL-WORLD MACHINE LEARNING, MODELS DO NOT FAIL BECAUSE THEY ARE TOO SIMPLE, THEY FAIL BECAUSE THEY **LEARN TOO MUCH**.\n",
    "- THIS PHENOMENON IS CALLED **OVERFITTING**.\n",
    "- REGULARIZATION IS A TECHNIQUE USED TO PREVENT **OVERFITTING** BY PENALIZING LARGE WEIGHTS ($\\theta$).\n",
    "- IT ADDS A `\"COMPLEXITY TERM\"` TO THE COST FUNCTION, FORCING THE MODEL TO KEEP PARAMETERS SMALL AND SIMPLE.\n",
    "\n",
    "> OVERFITTING OCCURS WHEN A MODEL:\n",
    "- MEMORIZES TRAINING DATA\n",
    "- LEARNS NOISE INSTEAD OF PATTERNS\n",
    "- PERFORMS WELL ON TRAIN DATA BUT FAILS ON UNSEEN DATA\n",
    "\n",
    "> REGULARIZATION IS A **CONTROL MECHANISM** THAT:\n",
    "- PENALIZES LARGE WEIGHTS\n",
    "- LIMITS MODEL COMPLEXITY\n",
    "- IMPROVES GENERALIZATION\n",
    "- INTRODUCES BIAS TO REDUCE VARIANCE\n",
    "\n",
    "LOGISTIC REGRESSION LEARNS PARAMETERS BY MINIMIZING A COST FUNCTION.\n",
    "\n",
    "WITHOUT REGULARIZATION:\n",
    "\n",
    "## REGULARIZATION IN LOGISTIC REGRESSION\n",
    "\n",
    "COST FUNCTION WITHOUT REGULARIZATION \n",
    "\n",
    "    J(Î¸) = -(1/m) Î£ [yÂ·log(hÎ¸(x)) + (1 âˆ’ y)Â·log(1 âˆ’ hÎ¸(x))]\n",
    "\n",
    "### THE THREE TYPES WE WILL IMPLEMENT:\n",
    "\n",
    "> **1. L2 REGULARIZATION (RIDGE)**\n",
    "> * **PENALTY:** SQUARED MAGNITUDE OF WEIGHTS ($\\theta_j^2$).\n",
    "> * **EFFECT:** SHRINKS ALL COEFFICIENTS TOWARDS ZERO EVENLY. GOOD FOR COLLINEARITY. SMOOTH DECISION BOUNDARY\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "> **2. L1 REGULARIZATION (LASSO)**\n",
    "> * **PENALTY:** ABSOLUTE VALUE OF WEIGHTS ($|\\theta_j|$).\n",
    "> * **EFFECT:** CAN SHRINK COEFFICIENTS TO EXACTLY ZERO. PERFORMS **FEATURE SELECTION**. CREATES SPARSE MODELS.\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "> **3. ELASTIC NET**\n",
    "> * **PENALTY:** A COMBINATION OF BOTH L1 AND L2.\n",
    "> * **EFFECT:** BALANCES SPARSITY (L1) AND STABILITY (L2).\n",
    "> * **MATH:**\n",
    ">   $$J(\\theta)_{reg} = J(\\theta) + r \\cdot (\\text{L1}) + (1-r) \\cdot (\\text{L2})$$\n",
    "\n",
    "---\n",
    "`NOTE`: WE DO NOT REGULARIZE THE BIAS TERM ($\\theta_0$). REGULARIZATION IS APPLIED **ONLY TO WEIGHTS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a937f9",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES\n",
    "WE WILL USE ONLY NUMPY, PANDAS, AND MATPLOTLIB/SEABORN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d976a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT NUMERICAL COMPUTATION LIBRARY\n",
    "import numpy as np\n",
    "\n",
    "# IMPORT DATA HANDLING LIBRARY\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT VISUALIZATION LIBRARIES\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cfb83",
   "metadata": {},
   "source": [
    "## VISUALIZATION STYLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b66ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP THE VISUALIZATION STYLE GLOBALLY TO MATCH YOUR REQUIRED FORMAT\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8db33",
   "metadata": {},
   "source": [
    "## LOADING DATA\n",
    "READING THE DATA FROM CSV FILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3becd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# IF HEADERS EXIST, REMOVE 'header=None'.\n",
    "\n",
    "# LOAD X DATA FROM CSV FILE\n",
    "X_raw = pd.read_csv('DATA/logisticX.csv', header=None).values\n",
    "\n",
    "# LOAD Y DATA FROM CSV FILE\n",
    "y_raw = pd.read_csv('DATA/logisticY.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645438fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SHAPE --> (100, 2)\n",
      "Y SHAPE --> (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# RESHAPING Y TO ENSURE IT IS A COLUMN VECTOR (M, 1)\n",
    "y_data = y_raw.reshape(-1, 1)\n",
    "\n",
    "# CHECKING SHAPE\n",
    "print(f\"X SHAPE --> {X_raw.shape}\")\n",
    "print(f\"Y SHAPE --> {y_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b01cb",
   "metadata": {},
   "source": [
    "## PREPROCESSING DATA\n",
    "WE WILL NORMALIZE THE DATA AND ADD AN INTERCEPT TERM (BIAS) TO X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecf9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NORMALIZATION (MIN-MAX SCALING)\n",
    "# THIS IS CRUCIAL FOR GRADIENT DESCENT CONVERGENCE\n",
    "# MATH: X_norm = (X - min) / (max - min)\n",
    "\n",
    "X_min = np.min(X_raw, axis = 0)\n",
    "X_max = np.max(X_raw, axis = 0)\n",
    "X_norm = (X_raw - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6034bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED X SHAPE: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# ADDING INTERCEPT TERM (COLUMN OF ONES) TO X\n",
    "# MATH: X = [1, x1, x2, ...]\n",
    "\n",
    "m = len(y_data)\n",
    "X_data = np.hstack((np.ones((m, 1)), X_norm))\n",
    "\n",
    "print(f\"PROCESSED X SHAPE: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe46a3a",
   "metadata": {},
   "source": [
    "## REGULARIZED LOGISTIC REGRESSION CLASS\n",
    "THIS CLASS HANDLES L1, L2, AND ELASTIC NET.\n",
    "\n",
    "`KEY UPDATES FROM PREVIOUS CODE`\n",
    "- **COST FUNCTION:** ADDED PENALTY TERMS.\n",
    "- **GRADIENT:** ADDED DERIVATIVE OF PENALTIES.\n",
    "    * DERIVATIVE OF $\\theta^2$ IS $2\\theta$.\n",
    "    * DERIVATIVE OF $|\\theta|$ IS $\\text{sgn}(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.5, iterations = 1000, penalty = 'None', lambda_ = 0.1, l1_ratio = 0.4):\n",
    "        self.lr = learning_rate\n",
    "        self.iter = iterations\n",
    "        self.penalty = penalty        # 'none', 'l1', 'l2', 'elasticnet'\n",
    "        self.lambda_ = lambda_        # REGULARIZATION STRENGTH\n",
    "        self.l1_ratio = l1_ratio      # FOR ElasticNet (0=L2, 1=L1)\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        epsilon = 1e-15\n",
    "        \n",
    "        # BASIC LOG LOSS\n",
    "        cost = -(1 / m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "        \n",
    "        # REGULARIZATION TERM (EXCLUDING BIAS theta[0])\n",
    "        # WE SLICE theta[1:] BECAUSE WE DON'T PENALIZE BIAS\n",
    "        \n",
    "        theta_noBias = self.theta[1:]\n",
    "        penalty_term = 0\n",
    "        \n",
    "        if self.penalty == 'L2':\n",
    "            penalty_term = (self.lambda_ / (2 * m)) * np.sum(np.square(theta_noBias))\n",
    "        \n",
    "        elif self.penalty == 'L1':\n",
    "            penalty_term = (self.lambda_ / m) * np.sum(np.abs(theta_noBias))\n",
    "       \n",
    "        elif self.penalty == 'ElasticNet':\n",
    "            L1 = np.sum(np.abs(theta_noBias))\n",
    "            L2 = np.sum(np.square(theta_noBias))\n",
    "            \n",
    "            # SKLEARN STYLE MIX: r * L1 + (1-r) * L2 (SCALED BY LAMBDA)\n",
    "            penalty_term = (self.lambda_ / m) * (self.l1_ratio * L1 + (1 - self.l1_ratio) *  0.5 * L2)\n",
    "            \n",
    "        return cost + penalty_term\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.theta = np.zeros((X.shape[1], 1))    \n",
    "        self.cost_history = []\n",
    "        m = len(y)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            #  HYPOTHESIS\n",
    "            h = self.sigmoid(np.dot(X, self.theta))\n",
    "            \n",
    "            # BASIC GRADIENT (BASE_GRAD)\n",
    "            # dJ/dTheta = (1/m) * X.T * (h - y)\n",
    "            gradient = (1 / m) * np.dot((X.T), (h - y))\n",
    "            \n",
    "            # ADDING REGULARIZATION TO GRADIENT\n",
    "            # WE CREATE A COPY OF THETA AND SET BIAS TO 0 SO IT DOESN'T GET PENALIZED\n",
    "            theta_reg = self.theta.copy()\n",
    "            theta_reg[0] = 0\n",
    "            \n",
    "            if self.penalty == 'L2':\n",
    "                # DERIVATIVE OF (lambda/2m * sum(theta^2)) -> (lambda/m) * theta\n",
    "                gradient += (self.lambda_ / m) * theta_reg\n",
    "            \n",
    "            elif self.penalty == 'L1':\n",
    "                # DERIVATIVE OF (lambda/m * sum|theta|) -> (lambda/m) * sign(theta)\n",
    "                gradient += (self.lambda_ / m) * np.sign(theta_reg)\n",
    "            \n",
    "            elif self.penalty == 'ElasticNet':\n",
    "                L1 = np.sign(theta_reg)\n",
    "                L2 = theta_reg\n",
    "                gradient += (self.lambda_ / m) * (self.l1_ratio * L1 + (1 - self.l1_ratio) * L2)\n",
    "                \n",
    "            # UPDATE\n",
    "            self.theta -= (self.lr * gradient)\n",
    "            self.cost_history.append(self.cost_function(X, y))\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"ITERATION {i} | COST: {self.cost_function(X, y):.6f}\") \n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
