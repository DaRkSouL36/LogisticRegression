# LOGISTIC REGRESSION FROM SCRATCH

<p align="center">
  <img src="https://img.shields.io/github/last-commit/DaRkSouL36/LogisticRegression?style=for-the-badge&color=blueviolet" />
  <img src="https://img.shields.io/github/repo-size/DaRkSouL36/LogisticRegression?style=for-the-badge&color=blue" />
  <img src="https://img.shields.io/github/languages/count/DaRkSouL36/LogisticRegression?style=for-the-badge&color=green" />
  <img src="https://img.shields.io/github/languages/top/DaRkSouL36/LogisticRegression?style=for-the-badge&color=orange" />
  <img src="https://img.shields.io/github/license/DaRkSouL36/LogisticRegression?style=for-the-badge&color=red" />
  <br><br>

<h1 align="center">PURE MATHEMATICS. NO BLACK BOXES.</h1>

> <p align="center"><b>MASTERING CLASSIFICATION FUNDAMENTALS BY BUILDING THEM FROM THE GROUND UP.</b></p>
> <p align="center"><b>BUILD IT. UNDERSTAND IT. VERIFY IT.</b></p>

<div style="text-align: justify;">
A COMPREHENSIVE IMPLEMENTATION OF LOGISTIC REGRESSION BUILT ENTIRELY FROM SCRATCH USING ONLY NUMPY. THIS PROJECT ELIMINATES THE ABSTRACTION OF LIBRARIES LIKE SCIKIT-LEARN TO REVEAL THE UNDERLYING CALCULUS, PROBABILITY THEORY, AND OPTIMIZATION MECHANICS DRIVING BINARY CLASSIFICATION. IT FEATURES CUSTOM IMPLEMENTATIONS OF GRADIENT DESCENT, DECISION BOUNDARY VISUALIZATION, REGULARIZATION TECHNIQUES (L1, L2, ELASTIC NET), AND IN-DEPTH METRIC ANALYSIS.
</div>

---

## üìñ TABLE OF CONTENTS

- [LOGISTIC REGRESSION FROM SCRATCH](#logistic-regression-from-scratch)
  - [üìñ TABLE OF CONTENTS](#-table-of-contents)
  - [üìò PROJECT OVERVIEW](#-project-overview)
  - [üí¨ FEATURES](#-features)
  - [üìÅ NOTEBOOKS BREAKDOWN](#-notebooks-breakdown)
    - [`ROOT DIRECTORY`](#root-directory)
  - [üß† MATHEMATICAL CONCEPTS IMPLEMENTED](#-mathematical-concepts-implemented)
    - [1. THE SIGMOID ACTIVATION](#1-the-sigmoid-activation)
    - [2. LOG LOSS COST FUNCTION](#2-log-loss-cost-function)
    - [3. GRADIENT DESCENT UPDATE RULE](#3-gradient-descent-update-rule)
    - [4. REGULARIZATION TERMS](#4-regularization-terms)
  - [üìä METHODS IMPLEMENTED](#-methods-implemented)
    - [üîÅ OPTIMIZATION \& TRAINING](#-optimization--training)
    - [üõ°Ô∏è REGULARIZATION TECHNIQUES](#Ô∏è-regularization-techniques)
    - [üìè EVALUATION METRICS (FROM SCRATCH)](#-evaluation-metrics-from-scratch)
  - [üìä VISUALIZATION HIGHLIGHTS](#-visualization-highlights)
  - [‚öôÔ∏è HOW IT WORKS](#Ô∏è-how-it-works)
  - [üèÅ USAGE](#-usage)
  - [üìà RESULTS AND CONCLUSION](#-results-and-conclusion)
    - [‚úî KEY OBSERVATIONS](#-key-observations)
  - [üöÄ FUTURE WORK](#-future-work)
  - [üìÑ LICENSE](#-license)
  - [üì¶ INSTALLATION INSTRUCTIONS](#-installation-instructions)

---

## üìò PROJECT OVERVIEW

THIS REPOSITORY SERVES AS A DEEP DIVE INTO THE MECHANICS OF BINARY CLASSIFICATION. INSTEAD OF RELYING ON `model.fit()`, THIS PROJECT MANUALLY IMPLEMENTS THE OPTIMIZATION LOOPS, LOG-LOSS CALCULATIONS, AND PARAMETER UPDATES. IT IS DESIGNED TO DEMONSTRATE THE MATHEMATICAL FIDELITY OF CUSTOM IMPLEMENTATIONS BY BENCHMARKING THEM AGAINST INDUSTRY-STANDARD LIBRARIES LIKE SCIKIT-LEARN AND VISUALIZING COMPLEX CONCEPTS LIKE PROBABILITY HEATMAPS AND COST SURFACES.

> THE PROJECT IS DESIGNED TO:
- REMOVE ALL ABSTRACTION
- EXPOSE THE TRUE MATHEMATICS BEHIND CLASSIFICATION
- DEMONSTRATE HOW DECISION BOUNDARIES, PROBABILITIES, AND METRICS EMERGE FROM CALCULUS AND LINEAR ALGEBRA

---

## üí¨ FEATURES

- **ZERO ML LIBRARIES:** BUILT STRICTLY WITH `NUMPY`, `PANDAS`, AND `MATPLOTLIB`.
- **ROBUST MATH:** HANDLES NUMERICAL STABILITY ISSUES (LIKE LOG(0)) USING EPSILON.
- **FULL REGULARIZATION SUITE:** IMPLEMENTS **L1 (LASSO)**, **L2 (RIDGE)**, AND **ELASTIC NET** FROM SCRATCH.
- **ADVANCED VISUALIZATION:** 3D COST SURFACES, PROBABILITY HEATMAPS, AND DYNAMIC DECISION BOUNDARIES.
- **METRIC ANALYSIS:** CUSTOM CALCULATION OF PRECISION, RECALL, F1-SCORE, AND CONFUSION MATRICES.
- **BENCHMARKING:** COMPARATIVE ANALYSIS AGAINST `SCIKIT-LEARN` TO VERIFY ACCURACY.

---

## üìÅ NOTEBOOKS BREAKDOWN

### `ROOT DIRECTORY`

- **LogisticRegression.ipynb** ‚Äì THE CORE IMPLEMENTATION COVERING THE BASIC LOGISTIC REGRESSION ALGORITHM, GRADIENT DESCENT, THRESHOLD ANALYSIS, AND METRIC CALCULATIONS.
  - CORE LOGISTIC REGRESSION IMPLEMENTATION
  - GRADIENT DESCENT TRAINING
  - DECISION BOUNDARY VISUALIZATION
  - THRESHOLD SHIFT ANALYSIS
  - METRICS & CONFUSION MATRIX
  - LEARNING RATE COMPARISONS
  - SCIKIT-LEARN VALIDATION
- **LogisticRegression_Regularization.ipynb** ‚Äì EXTENDS THE MODEL TO INCLUDE L1, L2, AND ELASTIC NET REGULARIZATION TO PREVENT OVERFITTING, FEATURING ADVANCED VISUALIZATIONS LIKE 3D COST LANDSCAPES.
  - L1 (LASSO) REGULARIZATION
  - L2 (RIDGE) REGULARIZATION
  - ELASTIC NET
  - COMPARISON WITH UNREGULARIZED MODEL
  - REGULARIZATION STRENGTH EFFECTS
  - VISUAL IMPACT ON DECISION BOUNDARY
- **DATA/** ‚Äì CONTAINS THE INPUT DATASETS `logisticX.csv` AND `logisticY.csv`.

---

## üß† MATHEMATICAL CONCEPTS IMPLEMENTED

### 1. THE SIGMOID ACTIVATION
$$g(z) = \frac{1}{1 + e^{-z}}$$

### 2. LOG LOSS COST FUNCTION
$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

### 3. GRADIENT DESCENT UPDATE RULE
$$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$

### 4. REGULARIZATION TERMS
- **L2 (RIDGE):** $\frac{\lambda}{2m} \sum \theta_j^2$
- **L1 (LASSO):** $\frac{\lambda}{m} \sum |\theta_j|$
- **ELASTIC NET:** MIXTURE OF L1 AND L2

---

## üìä METHODS IMPLEMENTED

### üîÅ OPTIMIZATION & TRAINING
- BATCH GRADIENT DESCENT
- LEARNING RATE TUNING & CONVERGENCE ANALYSIS 
- DECISION THRESHOLD TUNING (PRECISION-RECALL TRADE-OFF) 

### üõ°Ô∏è REGULARIZATION TECHNIQUES
- L2 REGULARIZATION (WEIGHT DECAY)
- L1 REGULARIZATION (SPARSITY INDUCTION)
- ELASTIC NET (BALANCED APPROACH)
  
### üìè EVALUATION METRICS (FROM SCRATCH)
- ACCURACY
- PRECISION
- RECALL
- F1-SCORE
- CONFUSION MATRIX

---

## üìä VISUALIZATION HIGHLIGHTS

- **DECISION BOUNDARY:** PLOTS THE LINEAR SEPARATOR OVER CLASS SCATTER PLOTS.
- **PROBABILITY HEATMAP:** VISUALIZES THE MODEL'S CONFIDENCE ACROSS THE ENTIRE FEATURE SPACE.
- **3D COST SURFACE:** INTERACTIVE VIEW OF THE COST FUNCTION LANDSCAPE AND THE OPTIMAL MINIMUM.
- **THRESHOLD TRADE-OFF:** CURVES SHOWING THE RELATIONSHIP BETWEEN PRECISION, RECALL, AND DECISION THRESHOLDS.
- **CONVERGENCE GRAPHS:** TRACKS COST REDUCTION OVER ITERATIONS FOR DIFFERENT LEARNING RATES.

---

## ‚öôÔ∏è HOW IT WORKS

1.  **DATA LOADING:** READS RAW CSV DATA AND PERFORMS MIN-MAX NORMALIZATION.
2.  **MODEL INITIALIZATION:** SETS HYPERPARAMETERS (LEARNING RATE, ITERATIONS, REGULARIZATION TYPE).
3.  **TRAINING LOOP:**
    * COMPUTES LINEAR PREDICTION ($z = \theta^T x$).
    * APPLIES SIGMOID ACTIVATION ($h = \sigma(z)$).
    * CALCULATES GRADIENTS WITH REGULARIZATION DERIVATIVE.
    * UPDATES WEIGHTS.
4.  **PREDICTION:** OUTPUTS PROBABILITIES AND CLASSIFIES BASED ON A THRESHOLD (DEFAULT 0.5).
5.  **VALIDATION:** COMPARES METRICS AND COEFFICIENTS AGAINST SCIKIT-LEARN.

---

## üèÅ USAGE

1.  **SELECT THE NOTEBOOK:** START WITH `LogisticRegression.ipynb` FOR BASICS OR THE REGULARIZATION NOTEBOOK FOR ADVANCED CONCEPTS.
2.  **RUN THE CELLS:** EXECUTE PREPROCESSING AND TRAINING STEPS.
3.  **EXPERIMENT:** MODIFY `lambda_`, `penalty`, OR `learning_rate` TO SEE HOW THE DECISION BOUNDARY SHIFTS.

---

## üìà RESULTS AND CONCLUSION

<div style="text-align: justify;">
THE CUSTOM IMPLEMENTATIONS ACHIEVED HIGH FIDELITY COMPARED TO SCIKIT-LEARN, VALIDATING THE ROBUSTNESS OF THE MATHEMATICS.

- **ACCURACY:** THE SCRATCH MODEL ACHIEVED `~89%` ACCURACY, MATCHING OR SLIGHTLY EXCEEDING THE SKLEARN BASELINE.
- **REGULARIZATION:** L2 AND ELASTIC NET SUCCESSFULLY SMOOTHED DECISION BOUNDARIES, REDUCING OVERFITTING RISKS.
- **COEFFICIENTS:** LEARNED WEIGHTS SHOWED CONSISTENT DIRECTIONALITY WITH SCIKIT-LEARN, CONFIRMING CORRECT GRADIENT DERIVATIONS.

THE PROJECT CONFIRMS THAT REGULARIZATION IS ESSENTIAL FOR CONTROLLED LEARNING AND THAT NUMERICAL STABILITY (EPSILON) IS CRITICAL FOR ROBUST IMPLEMENTATION.
</div>

### ‚úî KEY OBSERVATIONS
- MIN-MAX SCALING IS MORE STABLE FOR SIGMOID ACTIVATIONS THAN Z-SCORE.
- LOWER THRESHOLDS INCREASE RECALL (GOOD FOR MEDICAL/SAFETY CASES).
- HIGHER THRESHOLDS INCREASE PRECISION (GOOD FOR SPAM/FRAUD CASES).
- ELASTIC NET PROVIDES A BALANCED REGULARIZATION STRATEGY.

---

## üöÄ FUTURE WORK

- IMPLEMENT **MULTICLASS CLASSIFICATION** (ONE-VS-ALL).
- ADD **NEWTON'S METHOD** FOR FASTER OPTIMIZATION.
- IMPLEMENT OTHER MODELS FROM SCRATCH.

---

## üìÑ LICENSE

THIS PROJECT IS LICENSED UNDER THE [MIT LICENSE](LICENSE).

YOU ARE FREE TO USE, MODIFY, AND DISTRIBUTE THIS PROJECT WITH ATTRIBUTION.

---

## üì¶ INSTALLATION INSTRUCTIONS

> 1. **CLONE THE REPOSITORY**

---

`git clone https://github.com/DaRkSouL36/LogisticRegression`

`cd "LogisticRegression"`

---

> 2. **CREATE AND ACTIVATE VIRTUAL ENVIRONMENT**

---

`python -m venv VENV`

- LINUX/MAC
  
   `source VENV/bin/activate`

- ON WINDOWS
  
   `VENV\Scripts\activate`

---

> 3. **INSTALL REQUIRED DEPENDENCIES** 
---

`pip install -r REQUIREMENTS.txt`

---

<p>
  <a href="#logistic-regression-from-scratch">
    <img src="https://img.icons8.com/fluency/48/up.png" width="20px"/>
    <strong>RETURN</strong>
  </a>
</p>