# **MATHEMATICAL FOUNDATIONS OF LOGISTIC REGRESSION: MAXIMUM LIKELIHOOD ESTIMATION**

> **NOTE:** THIS DOCUMENT IS WRITTEN FOR RIGOROUS STUDY. IT ASSUMES FAMILIARITY WITH PROBABILITY THEORY AND CALCULUS. THE GOAL IS TO ESTABLISH THE **THEORETICAL LINK** BETWEEN PROBABILITY DISTRIBUTIONS AND THE COST FUNCTIONS WE OPTIMIZE IN MACHINE LEARNING.

---

## **1. THE DERIVATION OF THE MAXIMUM LIKELIHOOD FUNCTION**

### **THE CONCEPTUAL FRAMEWORK**

IN STATISTICAL INFERENCE, WE OFTEN OBSERVE DATA AND WISH TO ESTIMATE THE PARAMETERS OF THE DISTRIBUTION THAT GENERATED IT.

LET US DEFINE:
* $X = \{x^{(1)}, x^{(2)}, ..., x^{(m)}\}$ AS A SET OF $m$ INDEPENDENT AND IDENTICALLY DISTRIBUTED (I.I.D) RANDOM VARIABLES.
* THESE VARIABLES ARE DRAWN FROM A PROBABILITY DISTRIBUTION PARAMETERIZED BY $\theta$.
* $P(x ; \theta)$ REPRESENTS THE PROBABILITY DENSITY FUNCTION (OR MASS FUNCTION) OF OBSERVING $x$ GIVEN PARAMETER $\theta$.

### PROBABILITY VS LIKELIHOOD (CRITICAL DISTINCTION)

| CONCEPT | INTERPRETATION |
|------|-------------|
| **PROBABILITY** | $P(X \mid \theta)$ → DATA IS RANDOM |
| **LIKELIHOOD** | $L(\theta \mid X)$ → PARAMETERS ARE UNKNOWN |

> **SAME FUNCTION. DIFFERENT INTERPRETATION.**

---

### **DEFINING THE LIKELIHOOD**

WE WANT TO FIND THE PARAMETER $\theta$ THAT MAKES OUR OBSERVED DATA **MOST PROBABLE**.

THE **LIKELIHOOD FUNCTION** $L(\theta)$ IS DEFINED AS THE JOINT PROBABILITY OF OBSERVING ALL DATA POINTS IN OUR DATASET GIVEN THE PARAMETERS.

BECAUSE THE OBSERVATIONS ARE INDEPENDENT (I.I.D ASSUMPTION), THE JOINT PROBABILITY IS THE **PRODUCT** OF THE INDIVIDUAL PROBABILITIES:

$$L(\theta) = P(X ; \theta) = \prod_{i=1}^{m} P(x^{(i)} ; \theta)$$

### **THE PRINCIPLE OF MAXIMUM LIKELIHOOD ESTIMATION (MLE)**

THE GOAL OF MLE IS TO FIND THE PARAMETER $\hat{\theta}_{MLE}$ THAT MAXIMIZES THIS FUNCTION.

$$\hat{\theta}_{MLE} = \arg \max_{\theta} L(\theta) = \arg \max_{\theta} \prod_{i=1}^{m} P(x^{(i)} ; \theta)$$

> **INTERPRETATION:** WE ARE SEARCHING FOR THE SPECIFIC SETTING OF $\theta$ UNDER WHICH THE DATA WE ACTUALLY OBSERVED WOULD BE MOST LIKELY TO OCCUR.

---

## **2. USE AND SIGNIFICANCE OF MLE IN LOGISTIC REGRESSION**

IN LOGISTIC REGRESSION, WE ARE MODELING A **BINARY CLASSIFICATION** PROBLEM.

### **THE HYPOTHESIS AND THE BERNOULLI DISTRIBUTION**

WE ASSUME THAT FOR A GIVEN INPUT $x$, THE TARGET $y$ CAN TAKE VALUES $\{0, 1\}$. THIS FOLLOWS A **BERNOULLI DISTRIBUTION**.

$
y \sim \text{Bernoulli}(p)
$
`WHERE:`
$
p = \sigma(w^T x)
$

OUR HYPOTHESIS $h_\theta(x)$ OUTPUTS THE PROBABILITY THAT $y=1$:

$$h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} = P(y=1 | x; \theta)$$

CONSEQUENTLY:
* $P(y=1 | x; \theta) = h_\theta(x)$
* $P(y=0 | x; \theta) = 1 - h_\theta(x)$

### **COMPACT FORMULATION**

WE CAN WRITE THE PROBABILITY MASS FUNCTION FOR A SINGLE OBSERVATION $(x, y)$ IN A COMPACT MATHEMATICAL FORM:

$$P(y | x; \theta) = (h_\theta(x))^y (1 - h_\theta(x))^{1-y}$$

> **VERIFICATION:**
> * IF $y=1$: TERM BECOMES $(h_\theta(x))^1 (1 - h_\theta(x))^0 = h_\theta(x)$
> * IF $y=0$: TERM BECOMES $(h_\theta(x))^0 (1 - h_\theta(x))^1 = 1 - h_\theta(x)$

### **CONSTRUCTING THE LIKELIHOOD FOR LOGISTIC REGRESSION**

ASSUMING WE HAVE $m$ TRAINING EXAMPLES, THE LIKELIHOOD OF THE PARAMETERS $\theta$ GIVEN THE ENTIRE DATASET IS THE PRODUCT OF THE PROBABILITIES FOR EACH SAMPLE:

$$L(\theta) = \prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; \theta)$$

SUBSTITUTING OUR COMPACT BERNOULLI FORMULA:

$$L(\theta) = \prod_{i=1}^{m} \left[ (h_\theta(x^{(i)}))^{y^{(i)}} (1 - h_\theta(x^{(i)}))^{1-y^{(i)}} \right]$$

**SIGNIFICANCE:**
THIS EQUATION IS THE CORNERSTONE OF LOGISTIC REGRESSION TRAINING. IT TELLS US THAT OPTIMIZING THE MODEL IS EQUIVALENT TO FINDING $\theta$ THAT MAXIMIZES THIS PRODUCT.

---

## **3. DERIVATION OF THE LOG LOSS FUNCTION FROM MLE**

DIRECTLY MAXIMIZING THE LIKELIHOOD $L(\theta)$ IS COMPUTATIONALLY DIFFICULT BECAUSE IT INVOLVES THE PRODUCT OF MANY SMALL NUMBERS (PROBABILITIES BETWEEN 0 AND 1), WHICH LEADS TO **NUMERICAL UNDERFLOW**.

TO SOLVE THIS, WE APPLY A `MONOTONIC TRANSFORMATION`: THE **NATURAL LOGARITHM**.

### **STEP A: THE LOG-LIKELIHOOD**

WE DEFINE THE LOG-LIKELIHOOD FUNCTION $l(\theta)$ AS:

$$l(\theta) = \log(L(\theta))$$

SUBSTITUTING THE LIKELIHOOD EQUATION:

$$l(\theta) = \log \left( \prod_{i=1}^{m} (h_\theta(x^{(i)}))^{y^{(i)}} (1 - h_\theta(x^{(i)}))^{1-y^{(i)}} \right)$$

USING THE LOGARITHMIC PROPERTY $\log(ab) = \log(a) + \log(b)$:

$$l(\theta) = \sum_{i=1}^{m} \log \left( (h_\theta(x^{(i)}))^{y^{(i)}} (1 - h_\theta(x^{(i)}))^{1-y^{(i)}} \right)$$

USING THE PROPERTY $\log(a^b) = b \log(a)$:

$$l(\theta) = \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]$$

### **STEP B: FROM MAXIMIZATION TO MINIMIZATION**

IN OPTIMIZATION AND MACHINE LEARNING, WE TYPICALLY PREFER TO **MINIMIZE A COST FUNCTION** RATHER THAN MAXIMIZE AN OBJECTIVE FUNCTION (CONVENTION OF GRADIENT DESCENT).

MAXIMIZING THE LOG-LIKELIHOOD IS MATHEMATICALLY EQUIVALENT TO MINIMIZING THE **NEGATIVE LOG-LIKELIHOOD**.

$$J(\theta) = - \frac{1}{m} l(\theta)$$

> `NOTE:` THE $1/m$ FACTOR IS A SCALING CONSTANT TO MAKE THE COST INVARIANT TO DATASET SIZE; IT DOES NOT CHANGE THE LOCATION OF THE MINIMUM).

### **THE FINAL DERIVATION OF LOG LOSS**

SUBSTITUTING $l(\theta)$ INTO $J(\theta)$ GIVES US THE FAMOUS **BINARY CROSS-ENTROPY (LOG LOSS)** FUNCTION:

$$J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]$$

---

## **4. WHY USE THE LOGARITHM?**

YOU MAY ASK: `WHY INTRODUCE THE LOGARITHM? DOESN'T IT CHANGE THE ANSWER?`

> NO. THE LOGARITHM IS A **MONOTONICALLY INCREASING FUNCTION**. THEREFORE:

$$\arg \max_{\theta} L(\theta) = \arg \max_{\theta} \log(L(\theta))$$

THE $\theta$ THAT MAXIMIZES THE PROBABILITY ALSO MAXIMIZES THE LOG-PROBABILITY. WE USE IT FOR THREE CRITICAL REASONS:

### **I. NUMERICAL STABILITY**
* **THE PROBLEM:** PROBABILITIES ARE NUMBERS $\in [0, 1]$. IF YOU MULTIPLY 1000 PROBABILITIES LIKE $0.5^{1000}$, THE RESULT IS SO CLOSE TO ZERO THAT COMPUTERS TREAT IT AS $0$ (UNDERFLOW).
* **THE SOLUTION:** LOGS CONVERT MULTIPLICATION INTO ADDITION. SUMMING NEGATIVE NUMBERS IS NUMERICALLY STABLE.

### **II. MATHEMATICAL CONVENIENCE**
* **THE PROBLEM:** DIFFERENTIATING A PRODUCT OF $m$ TERMS (USING THE PRODUCT RULE) IS A NIGHTMARE.
* **THE SOLUTION:** DIFFERENTIATING A SUM OF $m$ TERMS IS TRIVIAL BECAUSE THE DERIVATIVE IS A LINEAR OPERATOR. <br>
$
\frac{d}{dx} \sum f(x) = \sum \frac{d}{dx} f(x)
$ 
<br> THIS MAKES DERIVING THE GRADIENTS FOR GRADIENT DESCENT POSSIBLE.

### **III. CONVEXITY**
* **THE SIGNIFICANCE:** FOR LOGISTIC REGRESSION, THE LOG LOSS FUNCTION $J(\theta)$ IS **CONVEX** WITH RESPECT TO THE PARAMETERS $\theta$.
* THIS GUARANTEES THAT IF WE RUN GRADIENT DESCENT, WE WILL CONVERGE TO THE **GLOBAL MINIMUM**, NOT GET STUCK IN A LOCAL MINIMUM. IF WE USED THE MEAN SQUARED ERROR (MSE) WITH THE SIGMOID FUNCTION, THE COST SURFACE WOULD BE NON-CONVEX AND "WAVY."

> **IN SUMMARY:**
> THE LOG LOSS IS NOT ARBITRARY. IT IS THE DIRECT MATHEMATICAL CONSEQUENCE OF ASSUMING A BERNOULLI DISTRIBUTION ON OUR DATA AND APPLYING THE PRINCIPLE OF MAXIMUM LIKELIHOOD.

---

## **5. WHY MLE IS FUNDAMENTAL HERE**

LOGISTIC REGRESSION IS A **PROBABILISTIC MODEL**
> - MLE ENSURES:
>   - WELL-CALIBRATED PROBABILITIES
>   - CONSISTENT STATISTICAL INTERPRETATION
>   - CONVEX OPTIMIZATION (GLOBAL MINIMUM)

---

## **6. INFORMATION-THEORETIC INTERPRETATION**

- LOG LOSS = **CROSS-ENTROPY**

$
H(p, q) = -\mathbb{E}_{p}[\log q]
$

> **WE ARE MINIMIZING INFORMATION MISMATCH**

---

## **7. LOGISTIC REGRESSION IS NOT JUST A CLASSIFIER.**  
  
IT IS:
> - A **BERNOULLI MLE MODEL**
> - A **PROBABILITY ESTIMATOR**
> - AN **INFORMATION-THEORETIC OPTIMIZER**
> - A **CONVEX STATISTICAL INFERENCE MACHINE**

--- 